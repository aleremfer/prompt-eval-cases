# Prompt Evaluation Cases

This repository includes a series of comparative prompt tests across different large language models (LLMs), such as GPT-4, Claude, Gemini, and Mistral.  
Each case focuses on how models interpret the same instruction in different contexts (English and Spanish), highlighting differences in:

- Output clarity
- Factual consistency
- Tone and cultural sensitivity
- Instruction following
- Ethical alignment

The goal is to show how prompt design impacts output behavior across models.

ðŸ§ª Datasets and annotations coming soon.
