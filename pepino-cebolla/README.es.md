# üß™ Caso de estudio QA y Factualidad: cuando ‚ÄúPepino‚Äù no es un pepino

### Un ejemplo real de ambig√ºedad sem√°ntica y alucinaci√≥n en modelos de lenguaje

---

## Resumen

Este breve caso de estudio documenta una interacci√≥n real con un modelo de lenguaje de gran tama√±o que produjo una respuesta coherente, pero factualmente incorrecta, debido a una ambig√ºedad sem√°ntica y a la ausencia de contexto expl√≠cito.

El ejemplo muestra c√≥mo los LLM pueden generar explicaciones plausibles en lugar de solicitar aclaraciones, y por qu√© este comportamiento es relevante en tareas de factualidad, QA sem√°ntico y evaluaci√≥n de sistemas de IA.

---

## El prompt

El usuario formul√≥ la siguiente pregunta (en espa√±ol):

> **¬´¬øEl Torpedo-66 es de pepino o de cebolla?¬ª**

No se proporcion√≥ ning√∫n contexto adicional.

---
*La transcripci√≥n de mi pregunta y su respuesta est√° disponible aqu√≠ en `full-dialogue.es.md`.*

---

## La respuesta del modelo

El modelo interpret√≥ la pregunta como si se refiriera a un bocadillo, asumiendo que:

- **pepino** = ingrediente (pepino / pepinillo)  
- **cebolla** = ingrediente  
- **Torpedo-66** = un bar o restaurante

Respondi√≥ con seguridad que *el Torpedo-66 llevaba pepinillo*, construyendo una interpretaci√≥n culinaria coherente, pero inexistente.

La respuesta era:

- r√°pida
- fluida,  
- segura,  
- muy veros√≠mil desde un punto de vista cultural,  
- pero... completamente falsa.

---

## La verdad sin matices (ground truth)

En realidad:

- **Torpedo 66** es el nombre de un club de f√∫tbol.
- Dicho club es de **Pepino**, un municipio real de la provincia de Toledo (Espa√±a).
- **Cebolla** es otro municipio muy cercano, tambi√©n en la provincia de Toledo.
- Tanto **Pepino** como **Cebolla** son *top√≥nimos*, no ingredientes.

La pregunta era ambigua de forma intencionada, pero resoluble desde el punto de vista factual con la interpretaci√≥n sem√°ntica correcta.

La pregunta se formul√≥ de manera verbal y sin pistas adicionales. El modelo resolvi√≥ la ambig√ºedad interpretando ‚Äúpepino‚Äù y ‚Äúcebolla‚Äù como ingredientes, a pesar de que ambas palabras tienen tambi√©n un uso geogr√°fico v√°lido.

---

## Por qu√© este ejemplo es relevante

Este caso resulta especialmente interesante porque combina varios factores de riesgo habituales en alucinaciones de modelos de lenguaje:

### 1. Ambig√ºedad l√©xica

Palabras como *pepino* y *cebolla* son:

- sustantivos comunes, y  
- nombres propios de lugares reales.

### 2. Entidad real de baja frecuencia

*Torpedo 66* no es una entidad conocida a nivel global, pero existe y es verificable.

### 3. Ausencia de contexto expl√≠cito

El prompt no mencionaba:

- f√∫tbol,  
- deporte,  
- geograf√≠a, ni  
- Espa√±a.

Esto oblig√≥ al modelo a elegir una interpretaci√≥n en lugar de pedir aclaraci√≥n.

---

## El problema de fondo: alucinaci√≥n frente a aclaraci√≥n

En lugar de solicitar una aclaraci√≥n (por ejemplo: *¬´¬øTe refieres al club de f√∫tbol?¬ª*), el modelo:

- resolvi√≥ internamente la ambig√ºedad,  
- optimiz√≥ la respuesta para que resultara plausible, y  
- gener√≥ una alucinaci√≥n culturalmente cre√≠ble.

Esto pone de manifiesto un rasgo estructural de los LLM:

> **Los modelos de lenguaje optimizan la coherencia del texto, no la cautela epistemol√≥gica.**

---

## C√≥mo habr√≠a sido una respuesta m√°s segura

Una respuesta orientada a factualidad podr√≠a haber sido:

> ¬´Si te refieres al club de f√∫tbol Torpedo 66, es de Pepino (Toledo).  
> Si te refieres a otra cosa, por favor, proporciona m√°s contexto.¬ª

Esta respuesta es menos ‚Äúfluida‚Äù, pero mucho m√°s robusta desde el punto de vista factual.

---

## Relevancia para QA y evaluaci√≥n de IA

Este ejemplo es √∫til para:

- detecci√≥n de alucinaciones,  
- desambiguaci√≥n de entidades,  
- pruebas de ambig√ºedad sem√°ntica,  
- evaluaci√≥n de factualidad,  
- dise√±o de procesos de QA con intervenci√≥n humana.

Demuestra c√≥mo prompts cotidianos y no adversariales pueden revelar limitaciones profundas en el comportamiento de los modelos.

---

## Conclusi√≥n

No se trataba de una pregunta trampa, sino de una ambig√ºedad natural que los humanos resolvemos mediante conocimiento del mundo y contexto impl√≠cito.

El fallo del modelo no fue aleatorio ni malicioso, sino sist√©mico.

Comprender y auditar este tipo de errores es esencial para desarrollar sistemas de lenguaje m√°s fiables y seguros.

---

## Licencia

Este caso de estudio se basa en una interacci√≥n real y puede reutilizarse con fines educativos y de investigaci√≥n.

**Licencia sugerida:**  
Creative Commons Attribution 4.0 International (CC BY 4.0)  
https://creativecommons.org/licenses/by/4.0/

**Etiquetas:**  
LLM, factualidad, alucinaciones, QA de IA, ambig√ºedad sem√°ntica
